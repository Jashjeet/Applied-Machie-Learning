{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzMyE386oaaV"
   },
   "source": [
    "**Fall 2019**\n",
    "\n",
    "**P556: Applied Machine Learning**\n",
    "\n",
    "**Assignment #1**\n",
    "\n",
    "**Due date: September 18, 2019. 11:59 PM**\n",
    "\n",
    "DO NOT CHANGE THE FUNCTION DEFINITIONS UNLESS APPROVED BY AN AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJz8ZxU7opMy"
   },
   "source": [
    "# Problem #1: Linear Regression\n",
    "\n",
    "##  Problem 1.1 (25 points)\n",
    "\n",
    "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
    "\n",
    "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
    "\n",
    "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW1xPyXPoonO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class linear_regression:\n",
    "    \n",
    "    def __init__(self, learning_rate, iterations, \n",
    "               fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate#alpha\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef#theta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\"\"\"\n",
    "Fit linear model.\n",
    "Parameters\n",
    "----------\n",
    "X : array-like, shape (n_samples, n_features)\n",
    "Training data\n",
    "y : array_like, shape (n_samples, n_targets)\n",
    "Target values.\n",
    "\"\"\"\n",
    "#pass\n",
    "        if self.normalize is True:\n",
    "            Xt=X.transpose()\n",
    "            mean=np.mean(Xt,axis=1).reshape(len(Xt),1)\n",
    "            std=np.std(Xt,axis=1).reshape(len(Xt),1)\n",
    "            Xt=(Xt-mean)/std#normalize\n",
    "            X=Xt.transpose()\n",
    "        m=len(X)\n",
    "        X = np.c_[np.ones(len(X)), X] # appended 1\n",
    "\n",
    "\n",
    "      \n",
    "        for i in range(iterations):\n",
    "            H=X @ self.coef\n",
    "            error=H.T-y.T\n",
    "            gradient=error@X*(1/m)\n",
    "            gradient=gradient.T\n",
    "            self.coef=self.coef-self.learning_rate*gradient\n",
    "        return self.coef\n",
    "\n",
    "  \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the linear model\n",
    "Parameters\n",
    "----------\n",
    "X : array_like, shape (n_samples, n_features)\n",
    "Samples.\n",
    "Returns\n",
    "-------\n",
    "C : array, shape (n_samples,)\n",
    "Returns predicted values.\n",
    "\"\"\"\n",
    "#pass\n",
    "        if self.normalize is True:\n",
    "            Xt=X.transpose()\n",
    "            mean=np.mean(Xt,axis=1).reshape(len(Xt),1)\n",
    "            std=np.std(Xt,axis=1).reshape(len(Xt),1)\n",
    "            Xt=(Xt-mean)/std#normalize\n",
    "            X=Xt.transpose()    \n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        Y_predicted=X@self.coef\n",
    "        return Y_predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Man3c1JrhVbr"
   },
   "source": [
    "## Problem 1.2 (10 points)\n",
    "\n",
    "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
    "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEFBL6WwhXUz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients are: \n",
      "\n",
      "[[ 2.27454802e+01]\n",
      " [-9.99833066e-01]\n",
      " [ 1.02928050e+00]\n",
      " [ 1.13582948e-02]\n",
      " [ 6.28799097e-01]\n",
      " [-1.85168686e+00]\n",
      " [ 2.71919641e+00]\n",
      " [-2.94781537e-01]\n",
      " [-3.09819062e+00]\n",
      " [ 1.92748412e+00]\n",
      " [-1.69691508e+00]\n",
      " [-2.25313646e+00]\n",
      " [ 5.79465015e-01]\n",
      " [-3.43475227e+00]]\n",
      "predictions for the values are: \n",
      "\n",
      "[[26.21099431]\n",
      " [24.08584697]\n",
      " [30.11255542]\n",
      " [12.64589313]\n",
      " [22.34925452]\n",
      " [19.87381584]\n",
      " [21.02053542]\n",
      " [21.94961943]\n",
      " [19.92170814]\n",
      " [21.31699869]\n",
      " [ 7.15578862]\n",
      " [17.90357197]\n",
      " [17.74072766]\n",
      " [ 6.14105429]\n",
      " [41.02316569]\n",
      " [34.01504635]\n",
      " [22.91128605]\n",
      " [38.40158554]\n",
      " [32.01712433]\n",
      " [23.9529684 ]\n",
      " [25.51876518]\n",
      " [25.68216798]\n",
      " [21.24052701]\n",
      " [31.44275339]\n",
      " [23.3439297 ]\n",
      " [10.88175682]\n",
      " [18.1526488 ]\n",
      " [19.61278372]\n",
      " [36.54220974]\n",
      " [21.77198036]\n",
      " [18.79881048]\n",
      " [18.22344507]\n",
      " [20.5125031 ]\n",
      " [24.92917348]\n",
      " [29.93839836]\n",
      " [19.73617965]\n",
      " [11.90239614]\n",
      " [24.98052119]\n",
      " [18.88860446]\n",
      " [16.24405856]\n",
      " [27.38549613]\n",
      " [21.88676881]\n",
      " [23.52653156]\n",
      " [16.015243  ]\n",
      " [24.16769547]\n",
      " [25.91743363]\n",
      " [21.06931461]\n",
      " [24.15230629]\n",
      " [11.19117198]\n",
      " [24.94327113]\n",
      " [22.88632871]\n",
      " [18.47522488]\n",
      " [25.33669566]\n",
      " [31.15602637]\n",
      " [14.29273262]\n",
      " [22.97218751]\n",
      " [21.95673666]\n",
      " [16.37888097]\n",
      " [15.58330293]\n",
      " [22.85251581]\n",
      " [18.71910774]\n",
      " [22.48106691]\n",
      " [34.18241177]\n",
      " [32.21071724]\n",
      " [18.3321712 ]\n",
      " [34.0967353 ]\n",
      " [19.30036863]\n",
      " [20.77810206]\n",
      " [20.20087904]\n",
      " [23.93388808]\n",
      " [23.77477754]\n",
      " [24.95923288]\n",
      " [31.95897781]\n",
      " [29.9937658 ]\n",
      " [26.31737998]\n",
      " [ 5.96947069]\n",
      " [37.75259607]\n",
      " [24.65940818]\n",
      " [28.19846976]\n",
      " [20.0553531 ]\n",
      " [29.67631376]\n",
      " [19.79502093]\n",
      " [19.39745242]\n",
      " [38.81006062]\n",
      " [40.2697462 ]\n",
      " [24.56264098]\n",
      " [25.81315924]\n",
      " [17.43760827]\n",
      " [27.13369661]\n",
      " [17.25129625]\n",
      " [16.70144987]\n",
      " [14.5011793 ]\n",
      " [25.82255212]\n",
      " [32.44788144]\n",
      " [22.69799623]\n",
      " [21.04674632]\n",
      " [ 1.18477122]\n",
      " [26.47482232]\n",
      " [16.97229365]\n",
      " [18.44940318]\n",
      " [26.34959766]\n",
      " [22.9277607 ]\n",
      " [33.33064907]\n",
      " [22.95691113]\n",
      " [28.61550467]\n",
      " [24.5887255 ]\n",
      " [ 7.94800013]\n",
      " [15.31456503]\n",
      " [22.83001684]\n",
      " [30.44564994]\n",
      " [34.25427577]\n",
      " [13.84900814]\n",
      " [21.17322509]\n",
      " [21.74480912]\n",
      " [13.6825133 ]\n",
      " [24.25809205]\n",
      " [ 6.71627116]\n",
      " [20.3749469 ]\n",
      " [ 9.86785774]\n",
      " [46.27758278]\n",
      " [32.06561349]\n",
      " [12.97232839]\n",
      " [18.50459063]\n",
      " [22.22901389]\n",
      " [24.34030346]\n",
      " [21.09250968]\n",
      " [35.85755151]\n",
      " [14.48162653]\n",
      " [21.65787964]\n",
      " [36.22262212]\n",
      " [19.92569808]\n",
      " [14.88142617]\n",
      " [14.91434379]\n",
      " [24.30892242]\n",
      " [16.0962764 ]\n",
      " [32.35993898]\n",
      " [26.17051689]\n",
      " [16.58354728]\n",
      " [25.18448411]\n",
      " [11.10959096]\n",
      " [15.71344247]\n",
      " [21.82136869]\n",
      " [33.68137291]\n",
      " [29.05642681]\n",
      " [26.75878841]\n",
      " [16.07175738]\n",
      " [32.53937367]\n",
      " [28.82910671]\n",
      " [15.38811172]\n",
      " [ 8.27754511]\n",
      " [29.1048588 ]\n",
      " [26.04610512]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    boston=load_boston()\n",
    "    \n",
    "    learning_rate=0.00001\n",
    "    iterations=3795000\n",
    "    \n",
    "    coef=np.zeros((14,1))\n",
    "\n",
    "\n",
    "    target=boston.target.reshape(len(boston.target),1)\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
    "                                                        target,\n",
    "                                                        test_size=.30,\n",
    "                                                        random_state=0)\n",
    "    \n",
    "    obj=linear_regression(learning_rate,iterations,True,True,coef)\n",
    "    coef=obj.fit(X_train,y_train)\n",
    "    y_pred=obj.predict(X_test)\n",
    "    print('coefficients are: \\n')\n",
    "    print(coef)\n",
    "    print('predictions for the values are: \\n')\n",
    "    print(y_pred)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FXqtD_KhZwV"
   },
   "source": [
    "## Problem 1.3 (10 points)\n",
    "\n",
    "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFlcvY_piKus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature which minimized the Mean Squared Error is: Column 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class linear_regression:\n",
    "    \n",
    "    def __init__(self, learning_rate, iterations, \n",
    "               fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate#alpha\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef#theta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.normalize is True:\n",
    "            Xt=X.transpose()\n",
    "            mean=np.mean(Xt,axis=1).reshape(len(Xt),1)\n",
    "            std=np.std(Xt,axis=1).reshape(len(Xt),1)\n",
    "            Xt=(Xt-mean)/std#normalize\n",
    "            X=Xt.transpose()\n",
    "        m=len(X)\n",
    "        X = np.c_[np.ones(len(X)), X] # appended 1\n",
    "\n",
    "\n",
    "      \n",
    "        for i in range(iterations):\n",
    "            H=X @ self.coef\n",
    "            error=H.T-y.T\n",
    "            gradient=error@X*(1/m)\n",
    "            gradient=gradient.T\n",
    "            self.coef=self.coef-self.learning_rate*gradient\n",
    "        return self.coef\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.normalize is True:\n",
    "            Xt=X.transpose()\n",
    "            mean=np.mean(Xt,axis=1).reshape(len(Xt),1)\n",
    "            std=np.std(Xt,axis=1).reshape(len(Xt),1)\n",
    "            Xt=(Xt-mean)/std#normalize\n",
    "            X=Xt.transpose()    \n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        Y_predicted=X@self.coef\n",
    "        return Y_predicted\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    boston=load_boston()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    target=boston.target.reshape(len(boston.target),1)\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
    "                                                        target,\n",
    "                                                        test_size=.30,\n",
    "                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    coef=np.zeros((2,1))\n",
    "    learning_rate=0.00001\n",
    "    iterations=3795000\n",
    "\n",
    "    col=np.size(X_test,axis=1)\n",
    "    c=-1\n",
    "    mse_min=9999\n",
    "    for i in range(col):\n",
    "        obj=linear_regression(learning_rate,iterations,True,True,coef)\n",
    "        coef=obj.fit(X_train[:,[i]],y_train)\n",
    "        y_pred=obj.predict(X_test[:,[0]])\n",
    "        mse=mean_squared_error(y_test,y_pred)\n",
    "        if mse<mse_min:\n",
    "            c=i\n",
    "            mse_min=mse\n",
    "        \n",
    "    print('The feature which minimized the Mean Squared Error is: Column '+str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcqYa1U3iRQ1"
   },
   "source": [
    "## Problem 1.4 (5 points)\n",
    "\n",
    "1. How do you interpret that a variable causes a model's mean square error to increase? (2 points)\n",
    "  - Answer: We need to study the variables which are located far away or are odd one. The points which have the largest difference from the mean of the dataset, will cause the mean square error to increase. This can be explained in terms of bias and variance. A high bias and high variance or a high bias and low variance of some particular data points can greatly affect the mean square error of a model to increase.\n",
    "  Mathematically, mean square error can be written as Sum of (Ypredicted - Yactual)^2. If the datapoint is far off the mean, it will cause it to increase.\n",
    "  \n",
    "2. Why we would want to normalize our variables? (1 point)\n",
    "  - Answer:\n",
    "  Normalize is important to scale the value of our dataset around a similar scale, so that, it is easy to make calculations. Also, if the range of one feature is very large, say, area of the house, which is typically in the range 1000 to 2000, whereas, the number of rooms in a house has a range of 1 to 5, the area feature will affect the result more, but that may not be true.\n",
    "  So, we normalize a dataset of a particulat feature X(j) using the following: (X(i)(j)-X(j)(mean))/X(j)(standard deviation). Where, (i) is the row, and (j) is the feature.\n",
    "  \n",
    "  In the Boston Data set, without normalization, the data range was exceeded and the result was NaN(Not a Number). It happened because some of the values were too large or too small to multiply. After normalization, the data returned was well within the accepted range.\n",
    "  \n",
    "3. A model fitted using the exact same split dataset with normalized values will generate the same coefficients as a model that was fitted using values that haven't been normalized. Clearly state whether that statement is true or false and explain your reasoning. (2 points)\n",
    "  - Answer:\n",
    "  A normalized dataset will generate different coefficients than a non normalized dataset. This is because, the range of the dataset has been limited to a few integers. Also, each feature will have almost similar importance in the calculation of the coefficients, unlike before when the larger valued features had larger impact. The coefficients will be scaled, but not the same.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJSot41BkMrB"
   },
   "source": [
    "# Problem 2: Binary Classification\n",
    "\n",
    "## Problem 2.1 (5 points)\n",
    "\n",
    "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
    "\n",
    "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
    "\n",
    "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96jFpGyMIFu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_dataset(k0, k1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \"\"\"\n",
    "    #pass\n",
    "    flag=1\n",
    "    m=k0+k1#Total no. of samples\n",
    "    d=6#not specified, so hardcoded\n",
    "    # Initialize Weights w\n",
    "    w=np.random.randint(1,10, size=(d+1,1))\n",
    "    flag=0\n",
    "\n",
    "    while(flag==0):\n",
    "        X1=np.random.randint(-100,100, size=(1,d))\n",
    "        X1temp=X1\n",
    "        X1 = np.c_[np.ones(len(X1)), X1]\n",
    "        H_theta=X1@w\n",
    "        if(H_theta<0):\n",
    "            flag=1\n",
    "       \n",
    "        \n",
    "    if (H_theta<0):\n",
    "        while(np.size(X1,axis=0)<k0):\n",
    "            b=np.random.randint(-100,100, size=(1,d))\n",
    "            btemp=b\n",
    "            b=np.c_[np.ones(len(b)), b]\n",
    "            if(b@w<0):\n",
    "                X1=np.append(X1,b,axis=0)\n",
    "                X1temp=np.append(X1temp,btemp,axis=0) \n",
    "    Y1=np.full((k0,1),-1)\n",
    "           \n",
    "    flag=0\n",
    "    while(flag==0):\n",
    "        X2=np.random.randint(-100,100, size=(1,d))\n",
    "        X2temp=X2\n",
    "        X2 = np.c_[np.ones(len(X2)), X2]\n",
    "        H_theta=X2@w\n",
    "        if(H_theta>0):\n",
    "            flag=1\n",
    "           \n",
    "    if (H_theta>0):\n",
    "        while(np.size(X2,axis=0)<k1):\n",
    "            b=np.random.randint(-100,100, size=(1,d))\n",
    "            btemp=b\n",
    "            b=np.c_[np.ones(len(b)), b]\n",
    "            if(b@w>0):\n",
    "                X2=np.append(X2,b,axis=0)\n",
    "                X2temp=np.append(X2temp,btemp,axis=0)\n",
    "                \n",
    "    Y2=np.full((k1,1),1) \n",
    "\n",
    "    X=np.append(X1temp,X2temp,axis=0)\n",
    "    Xfull=np.append(X1,X2,axis=0)\n",
    "    Y=np.append(Y1,Y2,axis=0)\n",
    "\n",
    "        \n",
    "    return(X,Y)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_dataset(k0, k1):\n",
    "    \n",
    "    #choosing the value of a,b,c in the equation of ax+by+c=0\n",
    "    a,b,c=np.random.randint(1,9,3)#initializing w\n",
    "    list1=[]#class0\n",
    "    target_y1=[]\n",
    "    list2=[]#class1\n",
    "    target_y2=[]\n",
    "    \n",
    "    def valueofy(m,x,c):\n",
    "        return ((m*x)+c)\n",
    "    \n",
    "    x00=0\n",
    "    y00=valueofy(-a/b,x00,-c/b)\n",
    "    \n",
    "    \n",
    "    x01=100\n",
    "    y01=valueofy(-a/b,x01,-c/b)\n",
    "\n",
    "    x,y=[x00,x01] ,[y00,y01]\n",
    "\n",
    "    \n",
    "    while(len(list1)<k0):\n",
    "        x1=np.random.randint(1,100)\n",
    "        y1=random.randrange(start=-500, stop=int(valueofy(-a/b,x1,-c/b)))\n",
    "        if [x1,y1] not in list1:\n",
    "            list1.append([x1,y1])\n",
    "            target_y1.append([-1])#=[0]*100\n",
    "            \n",
    "    for blue in list1:\n",
    "        plt.scatter(blue[0],blue[1],color='blue',s=5)\n",
    "#    print(list1)\n",
    "            \n",
    "    while(len(list2)<k1):\n",
    "        x2=np.random.randint(1,100)\n",
    "        y2=random.randrange(start=int(valueofy(-a/b,x2,-c/b)), stop=500)\n",
    "        if [x2,y2] not in list2:\n",
    "            list2.append([x2,y2])\n",
    "            target_y2.append([1])#=[0]*100\n",
    "            \n",
    "    for green in list2:\n",
    "        plt.scatter(green[0],green[1],color='green',s=5)\n",
    "#    print(list2)\n",
    "            \n",
    "        plt.plot(x,y,color='red')\n",
    "        plt.xlabel('X-Axis')\n",
    "        plt.ylabel('Y-Axis')\n",
    "#        var.set_ylabel('Y-Axis')\n",
    "    list1=np.asarray(list1)\n",
    "    list2=np.asarray(list2)\n",
    "#    Y=np.asarray([[-1]*k0])\n",
    "    X=np.concatenate([list1,list2])\n",
    "    Y=np.concatenate([target_y1,target_y2])\n",
    "\n",
    "    return(X,Y)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Parameters\n",
    "----------\n",
    "k0 : integer, number of samples for class 0\n",
    "k1 : integer, number of samples for class 1\n",
    "Returns\n",
    "-------\n",
    "X : array, shape (m, d), dimension numpy array where m is the number of\n",
    "samples and d is the number of features\n",
    "Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "\"\"\"\n",
    "#pass\n",
    "if __name__==\"__main__\":\n",
    "    var=generate_dataset(200,220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mw15CFikXI1"
   },
   "source": [
    "## Problem 2.2 (35 points)\n",
    "\n",
    "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
    "\n",
    "\n",
    "When you are given a dataset for which you do not have access the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
    "\n",
    "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
    "\n",
    "1. Randomly choose a $\\mathbf{w}$ to begin with.\n",
    "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
    "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
    "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
    "    and go back to the previous step (of randomly chosing a sample)\n",
    "    \n",
    "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
    "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
    "        - $y$ is the class label for the sample under consideration\n",
    "        - $\\mathbf{x}$ is the data-point under consideration\n",
    "    \n",
    "    \n",
    "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(X, y):\n",
    "    \n",
    "\"\"\"Predict using the binary classification model. Use the dataset generated\n",
    "using generate_data() as input for this function.\n",
    "Parameters\n",
    "----------\n",
    "X : array_like, shape (n_samples, n_features)\n",
    "Samples.\n",
    "y : array_like, shape (n_labels, 1)\n",
    "Returns\n",
    "-------\n",
    "w : array, shape (1,n_features)\n",
    "Returns the final weight vector w.\n",
    "\"\"\"\n",
    "#pass\n",
    "    d=np.size(X,axis=1)\n",
    "    m=np.size(X,axis=0)\n",
    "    w=np.random.randint(1,11, size=(d+1,1))\n",
    "    X=np.c_[np.ones(len(X)), X]\n",
    "    z=0\n",
    "    check_list=[0]*m\n",
    "    while(1==1):\n",
    "        z+=1\n",
    "        i=np.random.randint(0,np.size(X,axis=0))\n",
    "        H=X[i]@w\n",
    "        if ((H<0 and y[i]==-1)or(H>0 and y[i]==1)):\n",
    "            pass\n",
    "            check_list[i]=1\n",
    "        else:\n",
    "            t=y[i]*X[i]\n",
    "            t=t.reshape(len(t),1)\n",
    "            w=w+t\n",
    "        if(check_list==[1]*m):\n",
    "            break\n",
    "            \n",
    "    return(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen weights are: \n",
      "\n",
      "[[ 153.  171.  772.  873. 1161.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def generate_dataset1(k0, k1):\n",
    "    flag=1\n",
    "    m=k0+k1#Total no. of samples\n",
    "    d=4#not specified, so hardcoded\n",
    "    # Initialize Weights w\n",
    "    w=np.random.randint(1,10, size=(d+1,1))\n",
    "    flag=0\n",
    "\n",
    "    while(flag==0):\n",
    "        X1=np.random.randint(-100,100, size=(1,d))\n",
    "        X1temp=X1\n",
    "        X1 = np.c_[np.ones(len(X1)), X1]\n",
    "        H_theta=X1@w\n",
    "        if(H_theta<0):\n",
    "            flag=1\n",
    "       \n",
    "        \n",
    "    if (H_theta<0):\n",
    "        while(np.size(X1,axis=0)<k0):\n",
    "            b=np.random.randint(-100,100, size=(1,d))\n",
    "            btemp=b\n",
    "            b=np.c_[np.ones(len(b)), b]\n",
    "            if(b@w<0):\n",
    "                X1=np.append(X1,b,axis=0)\n",
    "                X1temp=np.append(X1temp,btemp,axis=0) \n",
    "    Y1=np.full((k0,1),-1)\n",
    "           \n",
    "    flag=0\n",
    "    while(flag==0):\n",
    "        X2=np.random.randint(-100,100, size=(1,d))\n",
    "        X2temp=X2\n",
    "        X2 = np.c_[np.ones(len(X2)), X2]\n",
    "        H_theta=X2@w\n",
    "        if(H_theta>0):\n",
    "            flag=1\n",
    "           \n",
    "    if (H_theta>0):\n",
    "        while(np.size(X2,axis=0)<k1):\n",
    "            b=np.random.randint(-100,100, size=(1,d))\n",
    "            btemp=b\n",
    "            b=np.c_[np.ones(len(b)), b]\n",
    "            if(b@w>0):\n",
    "                X2=np.append(X2,b,axis=0)\n",
    "                X2temp=np.append(X2temp,btemp,axis=0)\n",
    "                \n",
    "    Y2=np.full((k1,1),1) \n",
    "\n",
    "    X=np.append(X1temp,X2temp,axis=0)\n",
    "    Xfull=np.append(X1,X2,axis=0)\n",
    "    Y=np.append(Y1,Y2,axis=0)\n",
    "            \n",
    "    return(X,Y,w)\n",
    "def fit_line1(X, y):\n",
    "    d=np.size(X,axis=1)\n",
    "    m=np.size(X,axis=0)\n",
    "    w=np.random.randint(1,11, size=(d+1,1))\n",
    "    X=np.c_[np.ones(len(X)), X]\n",
    "    z=0\n",
    "    check_list=[0]*m\n",
    "    while(1==1):\n",
    "        z+=1\n",
    "        i=np.random.randint(0,np.size(X,axis=0))\n",
    "        H=X[i]@w\n",
    "        if ((H<0 and y[i]==-1)or(H>0 and y[i]==1)):\n",
    "            pass\n",
    "            check_list[i]=1\n",
    "        else:\n",
    "            t=y[i]*X[i]\n",
    "            t=t.reshape(len(t),1)\n",
    "            w=w+t\n",
    "        if(check_list==[1]*m):\n",
    "            break\n",
    "            \n",
    "    return(w.T)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    var=generate_dataset1(1000,1200)\n",
    "    w=fit_line1(var[0],var[1])\n",
    "    print('Chosen weights are: \\n')\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHc3t4e9MOxu"
   },
   "source": [
    "### Problem 2.3 (10 points)\n",
    "- Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)\n",
    "\n",
    "  - Answer: If the dataset, i.e. data points are completely linearly seperable, then the algorithm converges. It is because after sufficient iterations, the equation will correctly classify each point in the dataset in either Class0(-1) or Class1(+1). Had the points not been completely linearly seperable, the loop would have gone into an infinite state, and never stopped. In that case, some amount of misclassification threshold should be allowed.\n",
    "  In the above dataset that was generated, using the equation, all the points were linearly seperable. Hence the while loop stopped in time.\n",
    "  The logic is as follows:\n",
    "  1. Initialize coefficients and take a datapoint from the set.\n",
    "  2. Try to make a prediction.\n",
    "  3. If the prediction is correct, move to the next data point and repeat from steps 1. If the prediction is not correct, update the coefficients using the formula for the current misclassified datapoint.\n",
    "  4.If the required result is positive and predicted result is coming negative, the updated coefficients will updated positively.\n",
    "  5. Similarly, if the required result is negative and the predicted result is positive, the coefficients will be adjusted negatively.\n",
    "  6. When no more updated are required, which means all the data is classified correctly, it returns the coefficients.\n",
    "\n",
    "- What happens when the data is not linearly separable? What can be done to salvage the situation?\n",
    "\n",
    "  - Answer: If the data is not linearly seperable, it will never converge. We can introduce a misclassification threshold. This is the fraction of datapoints of error that can be accepted which are wrongly classified from Class0 to Class1 or vice versa. Keep it decreasing, so that maximum number of datasets are correctly classified."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-F19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
